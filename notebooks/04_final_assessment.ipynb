{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e60ab08",
   "metadata": {},
   "source": [
    "# 4. Final Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1083571",
   "metadata": {},
   "source": [
    "## 5.1 Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cb74e7",
   "metadata": {},
   "source": [
    "Let's get started. Below are the libraries used in this assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c241ada",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"../src\"))\n",
    "from visualization import print_CILP_results\n",
    "from models import Classifier, Embedder\n",
    "from training import run_training\n",
    "#import utils\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f30f447-41fd-4b60-9e11-d325ca379bb7",
   "metadata": {},
   "source": [
    "### 5.1.1 The Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d69e9d3",
   "metadata": {},
   "source": [
    "Next, let's load our classification model and call it `lidar_cnn`. If we take a moment to view the [assement_utils](assessment/assesment_utils.py), we can see the `Classifier` class used to construct the model. Please note the `get_embs` method, which we will be using to construct our cross-modal projector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89cf3584",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Classifier(\n",
       "  (embedder): Sequential(\n",
       "    (0): Conv2d(1, 50, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(50, 100, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): ReLU()\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(100, 200, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU()\n",
       "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (9): Conv2d(200, 200, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (10): ReLU()\n",
       "    (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (12): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=3200, out_features=100, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=100, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lidar_cnn = Classifier(1).to(device)\n",
    "lidar_cnn.load_state_dict(torch.load(\"../data/assessment/lidar_cnn.pt\", weights_only=True))\n",
    "# Do not unfreeze. Otherwise, it would be difficult to pass the assessment.\n",
    "for param in lidar_cnn.parameters():\n",
    "    lidar_cnn.requires_grad = False #changed vs original!\n",
    "lidar_cnn.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513b5415-e9ef-442e-a2fa-f4d58747cd62",
   "metadata": {},
   "source": [
    "### 5.1.2 The Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8313fe37",
   "metadata": {},
   "source": [
    "Below is the dataset we will be using in this assessment. It is similar to the dataset we used in the first few labs, but please note `self.classes`. Unlike the first lab where we predicted position, in this lab, we will determine whether the RGB or LiDAR we are evaluating contains a `cube` or a `sphere`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e1e26ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 64\n",
    "img_transforms = transforms.Compose([\n",
    "    transforms.Resize(IMG_SIZE),\n",
    "    transforms.ToTensor(),  # Scales data into [0,1]\n",
    "])\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, root_dir, start_idx, stop_idx):\n",
    "        self.classes = [\"cubes\", \"spheres\"]\n",
    "        self.root_dir = root_dir\n",
    "        self.rgb = []\n",
    "        self.lidar = []\n",
    "        self.class_idxs = []\n",
    "\n",
    "        for class_idx, class_name in enumerate(self.classes):\n",
    "            for idx in range(start_idx, stop_idx):\n",
    "                file_number = \"{:04d}\".format(idx)\n",
    "                rbg_img = Image.open(self.root_dir + class_name + \"/rgb/\" + file_number + \".png\")\n",
    "                rbg_img = img_transforms(rbg_img).to(device)\n",
    "                self.rgb.append(rbg_img)\n",
    "    \n",
    "                lidar_depth = np.load(self.root_dir + class_name + \"/lidar/\" + file_number + \".npy\")\n",
    "                lidar_depth = torch.from_numpy(lidar_depth[None, :, :]).to(torch.float32).to(device)\n",
    "                self.lidar.append(lidar_depth)\n",
    "\n",
    "                self.class_idxs.append(torch.tensor(class_idx, dtype=torch.float32)[None].to(device))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.class_idxs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        rbg_img = self.rgb[idx]\n",
    "        lidar_depth = self.lidar[idx]\n",
    "        class_idx = self.class_idxs[idx]\n",
    "        return rbg_img, lidar_depth, class_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2534d446-2f2c-4c44-a254-a3f34492685f",
   "metadata": {},
   "source": [
    "This data is available in the `/data/assessment` folder. Here is an example of one of the cubes. The images are small, but there is enough detail that our models will be able to tell the difference.\n",
    "\n",
    "<center><img src=\"data/assessment/cubes/rgb/0002.png\" /></center>\n",
    "\n",
    "Let's go ahead and load the data into a `DataLoader`. We'll set aside a few batches (`VALID_BATCHES`) for validation. The rest of the data will be used for training. We have `9999` images for each of the cube and sphere categories, so we'll multiply N times 2 to reflect the combined dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d49e3ec1-7598-44bf-8872-b130eb90b1e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19358\n",
      "640\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "VALID_BATCHES = 10\n",
    "N = 9999\n",
    "\n",
    "valid_N = VALID_BATCHES*BATCH_SIZE\n",
    "train_N = N - valid_N\n",
    "\n",
    "train_data = MyDataset(\"../data/assessment/\", 0, train_N)\n",
    "train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "valid_data = MyDataset(\"../data/assessment/\", train_N, N)\n",
    "valid_dataloader = DataLoader(valid_data, batch_size=BATCH_SIZE, shuffle=False, drop_last=True)\n",
    "\n",
    "N *= 2\n",
    "valid_N *= 2\n",
    "train_N *= 2\n",
    "\n",
    "print(len(train_data))\n",
    "print(len(valid_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613d3353",
   "metadata": {},
   "source": [
    "## 5.2 Contrastive Pre-training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b541da32",
   "metadata": {},
   "source": [
    "Before we create a cross-modal projection model, it would be nice to have a way to embed our RGB images as a starting point. Let's be efficient with our data and create a contrastive pre-training model. First, it would help to have a convolutional model. We've prepared a recommended architecture below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e4ad7f",
   "metadata": {},
   "source": [
    "The RGB data has `4` channels, and our LiDAR data has `1`. Let's initiate these embedding models respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab4b81c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_embedder = Embedder(4).to(device)\n",
    "lidar_embedder = Embedder(1).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4247201",
   "metadata": {},
   "source": [
    "Now that we have our embedding models, let's combine them into a `ContrastivePretraining` model.\n",
    "\n",
    "**TODO**: The `ContrastivePretraining` class below is almost done, but it has a few `FIXME`s. Please replace the FIXMEs to have a working model. Feel free to review notebook [02b_Contrastive_Pretraining.ipynb](02b_Contrastive_Pretraining.ipynb) for a hint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c09a3dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastivePretraining(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.img_embedder = img_embedder\n",
    "        self.lidar_embedder = lidar_embedder\n",
    "        self.cos = nn.CosineSimilarity()\n",
    "\n",
    "    def forward(self, rgb_imgs, lidar_depths):\n",
    "        img_emb = self.img_embedder(rgb_imgs)\n",
    "        lidar_emb = self.lidar_embedder(lidar_depths)\n",
    "\n",
    "        repeated_img_emb = img_emb.repeat_interleave(len(img_emb), dim=0)   # 0,0,0, ..., 1,1,1, ...\n",
    "        repeated_lidar_emb = lidar_emb.repeat(len(lidar_emb), 1)            # 0,1,2, ..., 0,1,2, ...\n",
    "\n",
    "        similarity = self.cos(repeated_img_emb, repeated_lidar_emb)\n",
    "        similarity = torch.unflatten(similarity, 0, (BATCH_SIZE, BATCH_SIZE))\n",
    "        similarity = (similarity + 1) / 2\n",
    "\n",
    "        logits_per_img = similarity\n",
    "        logits_per_lidar = similarity.T\n",
    "        return logits_per_img, logits_per_lidar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac62782",
   "metadata": {},
   "source": [
    "Time to put these models to the test! First, let's initialize the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7a4a6cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "moved CILP model to  cuda\n",
      "CILP trainable params: 1,853,950\n"
     ]
    }
   ],
   "source": [
    "CILP_model = ContrastivePretraining().to(device)\n",
    "print(\"moved CILP model to \", device)\n",
    "\n",
    "trainable_params = [p for p in CILP_model.parameters() if p.requires_grad]\n",
    "print(f\"CILP trainable params: {sum(p.numel() for p in trainable_params):,}\")\n",
    "\n",
    "optimizer = Adam(trainable_params, lr=1e-4)\n",
    "loss_img = nn.CrossEntropyLoss()\n",
    "loss_lidar = nn.CrossEntropyLoss()\n",
    "ground_truth = torch.arange(BATCH_SIZE, dtype=torch.long).to(device)\n",
    "epochs = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b411aa",
   "metadata": {},
   "source": [
    "Also we will use weights and biases to log our runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e04eb8cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: [wandb.login()] Loaded credentials for https://api.wandb.ai from /home/henrizeiler/.netrc.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mzeihenri\u001b[0m (\u001b[33mzeihenri-hasso-plattner-institut\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.25.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/henrizeiler/projects_python/Multimodal_Learning/notebooks/wandb/run-20260225_213106-fjky2l8p</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/zeihenri-hasso-plattner-institut/contrastive%20pre%20training/runs/fjky2l8p' target=\"_blank\">zany-thunder-1</a></strong> to <a href='https://wandb.ai/zeihenri-hasso-plattner-institut/contrastive%20pre%20training' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/zeihenri-hasso-plattner-institut/contrastive%20pre%20training' target=\"_blank\">https://wandb.ai/zeihenri-hasso-plattner-institut/contrastive%20pre%20training</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/zeihenri-hasso-plattner-institut/contrastive%20pre%20training/runs/fjky2l8p' target=\"_blank\">https://wandb.ai/zeihenri-hasso-plattner-institut/contrastive%20pre%20training/runs/fjky2l8p</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login()\n",
    "\n",
    "run = wandb.init(\n",
    "    project=\"contrastive pre training\",\n",
    "    config={\n",
    "        \"cilp_epochs\": epochs,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"n_train\": train_N,\n",
    "        \"n_val\": valid_N,\n",
    "        \"optimizer\": \"Adam\",\n",
    "        \"cilp_lr\": 1e-4,\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83430633",
   "metadata": {},
   "source": [
    "Before we can train the model, we should define a loss function to guide our model in learning.\n",
    "\n",
    "**TODO**: The `get_CILP_loss` function below is almost done. Do you remember the formula to calculate the loss? Please replace the `FIXME`s below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "181b9036-a22f-474d-b629-7a210dcbfab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_CILP_loss(batch):\n",
    "    rbg_img, lidar_depth, _ = batch\n",
    "    logits_per_img, logits_per_lidar = CILP_model(rbg_img, lidar_depth)\n",
    "    total_loss = (loss_img(logits_per_img, ground_truth) + loss_lidar(logits_per_lidar, ground_truth))/2\n",
    "    #print(\"Loss: \", total_loss)\n",
    "    return total_loss, logits_per_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93583438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean grad magnitude (sum over params): 2.093303e-04\n",
      "Mean weight delta after 1 step: 9.969963e-05\n"
     ]
    }
   ],
   "source": [
    "# Quick sanity check: confirm one optimizer step actually changes weights\n",
    "CILP_model.train()\n",
    "rgb_batch = torch.randn(BATCH_SIZE, 4, IMG_SIZE, IMG_SIZE, device=device)\n",
    "lidar_batch = torch.randn(BATCH_SIZE, 1, IMG_SIZE, IMG_SIZE, device=device)\n",
    "dummy_class = torch.zeros(BATCH_SIZE, 1, device=device)\n",
    "batch = (rgb_batch, lidar_batch, dummy_class)\n",
    "\n",
    "optimizer.zero_grad()\n",
    "loss_before, _ = get_CILP_loss(batch)\n",
    "loss_before.backward()\n",
    "\n",
    "grad_norm = 0.0\n",
    "for p in CILP_model.parameters():\n",
    "    if p.grad is not None:\n",
    "        grad_norm += p.grad.detach().abs().mean().item()\n",
    "\n",
    "w_before = CILP_model.img_embedder.conv[0].weight.detach().clone()\n",
    "optimizer.step()\n",
    "w_after = CILP_model.img_embedder.conv[0].weight.detach()\n",
    "delta = (w_after - w_before).abs().mean().item()\n",
    "\n",
    "print(f\"Mean grad magnitude (sum over params): {grad_norm:.6e}\")\n",
    "print(f\"Mean weight delta after 1 step: {delta:.6e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b182b887-ef15-4cf1-9916-161f8db05a24",
   "metadata": {},
   "source": [
    "Next, it's time to train. If the above `TODO`s were completed correctly, the loss should be under `3.2`. Are the values along the diagional close to `1`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4db7204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Train Loss: 3.080447541342841 \n",
      "Similarity:\n",
      "tensor([[0.9976, 0.0739, 0.9517,  ..., 0.2866, 0.5012, 0.1895],\n",
      "        [0.0774, 0.9933, 0.0877,  ..., 0.5959, 0.4071, 0.6787],\n",
      "        [0.9595, 0.0719, 0.9971,  ..., 0.2317, 0.4527, 0.1425],\n",
      "        ...,\n",
      "        [0.2899, 0.6100, 0.2556,  ..., 0.9958, 0.3441, 0.6587],\n",
      "        [0.5287, 0.3884, 0.4504,  ..., 0.3668, 0.9973, 0.8251],\n",
      "        [0.3024, 0.5784, 0.2307,  ..., 0.5898, 0.9269, 0.9721]],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Valid Loss: 3.19351463568838 \n",
      "Similarity:\n",
      "tensor([[0.9953, 0.8765, 0.5017,  ..., 0.5064, 0.5460, 0.9880],\n",
      "        [0.8633, 0.9952, 0.4986,  ..., 0.2495, 0.2731, 0.8715],\n",
      "        [0.4522, 0.4615, 0.9951,  ..., 0.5610, 0.4382, 0.4017],\n",
      "        ...,\n",
      "        [0.5125, 0.2639, 0.5176,  ..., 0.9975, 0.9853, 0.4914],\n",
      "        [0.5608, 0.2914, 0.4119,  ..., 0.9790, 0.9980, 0.5496],\n",
      "        [0.9916, 0.8762, 0.4030,  ..., 0.4863, 0.5497, 0.9967]],\n",
      "       device='cuda:0')\n",
      "Epoch 1\n",
      "Train Loss: 3.028940698202965 \n",
      "Similarity:\n",
      "tensor([[0.9958, 0.8960, 0.2808,  ..., 0.7357, 0.8084, 0.8633],\n",
      "        [0.8858, 0.9982, 0.2253,  ..., 0.6917, 0.8192, 0.6272],\n",
      "        [0.3069, 0.2291, 0.9992,  ..., 0.2925, 0.2841, 0.3863],\n",
      "        ...,\n",
      "        [0.6519, 0.6375, 0.3210,  ..., 0.9934, 0.9454, 0.8053],\n",
      "        [0.7767, 0.8754, 0.2831,  ..., 0.9293, 0.9850, 0.6949],\n",
      "        [0.8402, 0.6761, 0.3179,  ..., 0.8592, 0.8285, 0.9897]],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Valid Loss: 3.183990014226813 \n",
      "Similarity:\n",
      "tensor([[0.9946, 0.8046, 0.3599,  ..., 0.5045, 0.5756, 0.9770],\n",
      "        [0.8236, 0.9976, 0.3637,  ..., 0.2510, 0.2895, 0.8191],\n",
      "        [0.3113, 0.3549, 0.9980,  ..., 0.5217, 0.3996, 0.2586],\n",
      "        ...,\n",
      "        [0.4700, 0.2597, 0.5302,  ..., 0.9985, 0.9786, 0.4707],\n",
      "        [0.5440, 0.2915, 0.4255,  ..., 0.9854, 0.9982, 0.5509],\n",
      "        [0.9906, 0.8252, 0.2650,  ..., 0.4903, 0.5748, 0.9989]],\n",
      "       device='cuda:0')\n",
      "Epoch 2\n",
      "Train Loss: 3.023601461998859 \n",
      "Similarity:\n",
      "tensor([[0.9966, 0.2144, 0.2655,  ..., 0.6269, 0.8775, 0.5580],\n",
      "        [0.2170, 0.9977, 0.9686,  ..., 0.4435, 0.2162, 0.6453],\n",
      "        [0.2884, 0.9784, 0.9991,  ..., 0.3733, 0.2554, 0.8002],\n",
      "        ...,\n",
      "        [0.5903, 0.4263, 0.3669,  ..., 0.9984, 0.8462, 0.2973],\n",
      "        [0.8487, 0.2161, 0.2456,  ..., 0.8385, 0.9990, 0.4750],\n",
      "        [0.5784, 0.6866, 0.8050,  ..., 0.3039, 0.4709, 0.9982]],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Valid Loss: 3.181900952991686 \n",
      "Similarity:\n",
      "tensor([[0.9967, 0.7837, 0.3613,  ..., 0.4991, 0.5784, 0.9546],\n",
      "        [0.8018, 0.9984, 0.4321,  ..., 0.2773, 0.2819, 0.8532],\n",
      "        [0.3541, 0.4279, 0.9986,  ..., 0.5092, 0.3850, 0.3278],\n",
      "        ...,\n",
      "        [0.5073, 0.2828, 0.5239,  ..., 0.9990, 0.9739, 0.4815],\n",
      "        [0.5752, 0.2804, 0.4092,  ..., 0.9786, 0.9992, 0.5293],\n",
      "        [0.9738, 0.8494, 0.3234,  ..., 0.4775, 0.5378, 0.9992]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    CILP_model.train()\n",
    "    train_loss = 0\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        #print(\"step: \", step) DOES STEP\n",
    "        optimizer.zero_grad()\n",
    "        loss, logits_per_img = get_CILP_loss(batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    epoch_train_loss = train_loss / step\n",
    "    print_CILP_results(epoch, epoch_train_loss, logits_per_img, is_train=True)\n",
    "\n",
    "    CILP_model.eval()\n",
    "    valid_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for step, batch in enumerate(valid_dataloader):\n",
    "            loss, logits_per_img = get_CILP_loss(batch)\n",
    "            valid_loss += loss.item()\n",
    "    epoch_val_loss = valid_loss / step\n",
    "    print_CILP_results(epoch, epoch_val_loss, logits_per_img, is_train=False)\n",
    "\n",
    "    wandb.log({\n",
    "        \"cilp/train_loss\": epoch_train_loss,\n",
    "        \"cilp/val_loss\": epoch_val_loss,\n",
    "    }, step=epoch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cb8027-4ffb-4ff8-81a2-395f5c76ffcb",
   "metadata": {},
   "source": [
    "When complete, please freeze the model. We will assess this model with our cross-model projection model, and if this model is altered during cross-model projection training, it may not pass!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea35a356-4536-494b-98c9-806dced9ccdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in CILP_model.parameters():\n",
    "    CILP_model.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7bf714",
   "metadata": {},
   "source": [
    "## 5.3 Cross-Modal Projection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674845f5",
   "metadata": {},
   "source": [
    "Now that we have a way to embed our image data, let's move on to cross-modal projection. \n",
    "\n",
    "**TODO**: Let's jump right in and create the projector. What should be the dimensions into the model, and what should be the dimensions out of the model? A hint to the first `FIXME` can be found in section [#5.2-Contrastive-Pre-training](#5.2-Contrastive-Pre-training) in the `Embedder` class. A hint to the second `FIXME` can be found in the [assessment/assesment_utils.py](assessment/assesment_utils.py) file in the `Classifier` class. The dimensions of the second `FIXME` should be the same size as the output of the `get_embs` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6427d43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The projector maps from the constrastive pretraining models latent space to that of our lidar cnn model.\n",
    "#Even when the dimensionality of the embeddings match we still need to align the latent spaces\n",
    "# -> the first fixme needs to be replaced by the CILP embedding size (output off the embedders)\n",
    "# -> the second one depends on the output of the embedder the classifier was initially trained with\n",
    "\n",
    "projector = nn.Sequential(\n",
    "    nn.Linear(200, 1000),   #maybe I should not turnn these into dense embeddings, lets see\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(1000, 500),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(500, 200 * 4 * 4)\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81457189",
   "metadata": {},
   "source": [
    "Next, let's define the loss function for training the `projector`.\n",
    "\n",
    "**TODO**: What was the loss function for estimating projection embeddings? Please replace the `FIXME` below. Review notebook [03a_Projection.ipynb](03a_Projection.ipynb) section 3.2 for a hint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe942748-a017-4725-8350-86cbbbe6e838",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_projector_loss(model, batch):\n",
    "    rbg_img, lidar_depth, class_idx = batch\n",
    "    imb_embs = CILP_model.img_embedder(rbg_img)\n",
    "    lidar_emb = lidar_cnn.get_embs(lidar_depth)\n",
    "    pred_lidar_embs = model(imb_embs)\n",
    "    return nn.MSELoss()(pred_lidar_embs, lidar_emb), pred_lidar_embs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759ac873-88ce-4d93-8000-89f77eac929e",
   "metadata": {},
   "source": [
    "The `projector` will take a little while to train, but at the end of it, should reach a validation loss around 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "19e75d32-bceb-4e8a-a785-14a9aa4348fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 | Train: 3.4750  Val: 3.2427\n",
      "Epoch   1 | Train: 3.1597  Val: 3.1567\n",
      "Epoch   2 | Train: 3.0925  Val: 3.0813\n",
      "Epoch   3 | Train: 3.0347  Val: 3.0276\n",
      "Epoch   4 | Train: 2.9249  Val: 2.8820\n",
      "Epoch   5 | Train: 2.7349  Val: 2.6280\n",
      "Epoch   6 | Train: 2.5874  Val: 2.4948\n",
      "Epoch   7 | Train: 2.4050  Val: 2.4433\n",
      "Epoch   8 | Train: 2.3128  Val: 2.3548\n",
      "Epoch   9 | Train: 2.2244  Val: 2.3243\n",
      "Epoch  10 | Train: 2.1567  Val: 2.2408\n",
      "Epoch  11 | Train: 2.1000  Val: 2.1560\n",
      "Epoch  12 | Train: 2.0552  Val: 2.2638\n",
      "Epoch  13 | Train: 2.0151  Val: 2.0913\n",
      "Epoch  14 | Train: 1.9818  Val: 2.1904\n",
      "Epoch  15 | Train: 1.9650  Val: 2.1680\n",
      "Epoch  16 | Train: 1.9437  Val: 2.1411\n",
      "Epoch  17 | Train: 1.9106  Val: 1.9241\n",
      "Epoch  18 | Train: 1.9129  Val: 1.9698\n",
      "Epoch  19 | Train: 1.8825  Val: 1.9649\n",
      "Epoch  20 | Train: 1.8799  Val: 1.8895\n",
      "Epoch  21 | Train: 1.8322  Val: 1.9870\n",
      "Epoch  22 | Train: 1.8231  Val: 1.9491\n",
      "Epoch  23 | Train: 1.8217  Val: 1.8921\n",
      "Epoch  24 | Train: 1.7990  Val: 1.8701\n",
      "Epoch  25 | Train: 1.8136  Val: 1.8580\n",
      "Epoch  26 | Train: 1.7699  Val: 1.7926\n",
      "Epoch  27 | Train: 1.7671  Val: 1.8445\n",
      "Epoch  28 | Train: 1.7470  Val: 1.9408\n",
      "Epoch  29 | Train: 1.7494  Val: 2.0089\n",
      "Epoch  30 | Train: 1.7354  Val: 1.8169\n",
      "Epoch  31 | Train: 1.7349  Val: 1.8194\n",
      "Epoch  32 | Train: 1.7282  Val: 1.7716\n",
      "Epoch  33 | Train: 1.7305  Val: 2.0573\n",
      "Epoch  34 | Train: 1.7304  Val: 1.8213\n",
      "Epoch  35 | Train: 1.7081  Val: 1.8741\n",
      "Epoch  36 | Train: 1.6959  Val: 1.8548\n",
      "Epoch  37 | Train: 1.6889  Val: 1.8785\n",
      "Epoch  38 | Train: 1.6814  Val: 1.7712\n",
      "Epoch  39 | Train: 1.6648  Val: 1.7663\n"
     ]
    }
   ],
   "source": [
    "epochs = 40\n",
    "optimizer = torch.optim.Adam(projector.parameters())\n",
    "run.config.update({\"projector_epochs\": epochs, \"projector_optimizer\": \"Adam\"})\n",
    "\n",
    "train_losses, val_losses, _, _ = run_training(\n",
    "    projector, optimizer, epochs, train_dataloader, valid_dataloader, device, get_projector_loss, False\n",
    ")\n",
    "\n",
    "for epoch, (t_loss, v_loss) in enumerate(zip(train_losses, val_losses)):\n",
    "    wandb.log({\"projector/train_loss\": t_loss, \"projector/val_loss\": v_loss}, step=epoch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352b916d-8bcf-43f0-8c06-2d12302fef05",
   "metadata": {},
   "source": [
    "Time to bring it together. Let's create a new model `RGB2LiDARClassifier` where we can use our projector with the pre-trained `lidar_cnn` model.\n",
    "\n",
    "**TODO**: Please fix the `FIXME`s below. Which `embedder` should we be using from our `CILP_model`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ce38f00d-70a5-4d07-af19-58afebb49063",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RGB2LiDARClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.projector = projector\n",
    "        self.img_embedder = CILP_model.img_embedder\n",
    "        self.shape_classifier = lidar_cnn\n",
    "    \n",
    "    def forward(self, imgs):\n",
    "        img_encodings = self.img_embedder(imgs)\n",
    "        proj_lidar_embs = self.projector(img_encodings)\n",
    "        return self.shape_classifier(data_embs=proj_lidar_embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9978420f-2887-4b38-82c0-b3888541d3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_classifier = RGB2LiDARClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade3fd01-8364-4165-b344-a48c286dbe92",
   "metadata": {},
   "source": [
    "Before we train this model, let's see how it does out of the box. We'll create a function `get_correct` that we can use to calculate the number of classifications that were correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9f470060-3e8e-4b43-b56e-b74fd0160c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correct(output, y):\n",
    "    zero_tensor = torch.tensor([0]).to(device)\n",
    "    pred = torch.gt(output, zero_tensor)\n",
    "    correct = pred.eq(y.view_as(pred)).sum().item()\n",
    "    return correct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc69ad75-fe12-43de-9758-94047cfa3a3d",
   "metadata": {},
   "source": [
    "Next, we can make a `get_valid_metrics` function to calculate the model's accuracy with the validation dataset. If done correctly, the accuracy should be above `.70`, or 70%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d50b467c-7a11-492b-aa69-bb3695b27e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss: 1.2299 | Accuracy 0.8484\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.2299052242189645, 0.8484375)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_valid_metrics():\n",
    "    my_classifier.eval()\n",
    "    correct = 0\n",
    "    batch_correct = 0\n",
    "    total_loss = 0\n",
    "    for step, batch in enumerate(valid_dataloader):\n",
    "        rbg_img, _, class_idx = batch\n",
    "        output = my_classifier(rbg_img)\n",
    "        loss = nn.BCEWithLogitsLoss()(output, class_idx)\n",
    "        batch_correct = get_correct(output, class_idx)\n",
    "        correct += batch_correct\n",
    "        total_loss += loss.item()\n",
    "    avg_loss = total_loss / (step + 1)\n",
    "    accuracy = correct / valid_N\n",
    "    print(f\"Valid Loss: {avg_loss:2.4f} | Accuracy {accuracy:2.4f}\")\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "get_valid_metrics()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85538953-0ee3-4168-898e-baaa8f8c30e6",
   "metadata": {},
   "source": [
    "Finally, let's fine-tune the completed model. Since `CILP` and `lidar_cnn` are frozen, this should only change the `projector` part of the model. Even so, the model should achieve a validation accuracy of above `.95` or 95%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "10dd4664-7520-47a2-92d0-b455d9678f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5954 | Accuracy 0.6655\n",
      "Valid Loss: 0.2562 | Accuracy 0.9078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 0 that is less than the current step 2. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1 that is less than the current step 2. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 0 that is less than the current step 39. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1851 | Accuracy 0.9362\n",
      "Valid Loss: 0.1218 | Accuracy 0.9672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1 that is less than the current step 39. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0387 | Accuracy 0.9873\n",
      "Valid Loss: 0.0185 | Accuracy 0.9922\n",
      "Train Loss: 0.0245 | Accuracy 0.9911\n",
      "Valid Loss: 0.0337 | Accuracy 0.9844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 2 that is less than the current step 39. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 3 that is less than the current step 39. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0192 | Accuracy 0.9929\n",
      "Valid Loss: 0.0012 | Accuracy 1.0000\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "optimizer = torch.optim.Adam(my_classifier.parameters())\n",
    "run.config.update({\"finetune_epochs\": epochs, \"finetune_optimizer\": \"Adam\"})\n",
    "\n",
    "my_classifier.train()\n",
    "for epoch in range(epochs):\n",
    "    correct = 0\n",
    "    batch_correct = 0\n",
    "    train_loss_total = 0\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        rbg_img, _, class_idx = batch\n",
    "        output = my_classifier(rbg_img)\n",
    "        loss = nn.BCEWithLogitsLoss()(output, class_idx)\n",
    "        batch_correct = get_correct(output, class_idx)\n",
    "        correct += batch_correct\n",
    "        train_loss_total += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    train_accuracy = correct / train_N\n",
    "    avg_train_loss = train_loss_total / (step + 1)\n",
    "    print(f\"Train Loss: {avg_train_loss:2.4f} | Accuracy {train_accuracy:2.4f}\")\n",
    "    val_loss, val_accuracy = get_valid_metrics()\n",
    "    wandb.log({\n",
    "        \"finetune/train_loss\": avg_train_loss,\n",
    "        \"finetune/train_accuracy\": train_accuracy,\n",
    "        \"finetune/val_loss\": val_loss,\n",
    "        \"finetune/val_accuracy\": val_accuracy,\n",
    "    }, step=epoch)\n",
    "\n",
    "# Log final performance as summary metrics\n",
    "wandb.summary[\"final_val_accuracy\"] = val_accuracy\n",
    "wandb.summary[\"final_val_loss\"] = val_loss\n",
    "wandb.summary[\"final_train_accuracy\"] = train_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ac6310f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>cilp/train_loss</td><td>█▂▁</td></tr><tr><td>cilp/val_loss</td><td>█▂▁</td></tr><tr><td>projector/train_loss</td><td>██▇▆▆▅▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>projector/val_loss</td><td>██▇▆▅▅▄▄▄▃▄▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▂▂▁▁▁▃▁▂▁▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>cilp/train_loss</td><td>3.0236</td></tr><tr><td>cilp/val_loss</td><td>3.1819</td></tr><tr><td>final_train_accuracy</td><td>0.99287</td></tr><tr><td>final_val_accuracy</td><td>1</td></tr><tr><td>final_val_loss</td><td>0.00122</td></tr><tr><td>projector/train_loss</td><td>1.66481</td></tr><tr><td>projector/val_loss</td><td>1.76633</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">zany-thunder-1</strong> at: <a href='https://wandb.ai/zeihenri-hasso-plattner-institut/contrastive%20pre%20training/runs/fjky2l8p' target=\"_blank\">https://wandb.ai/zeihenri-hasso-plattner-institut/contrastive%20pre%20training/runs/fjky2l8p</a><br> View project at: <a href='https://wandb.ai/zeihenri-hasso-plattner-institut/contrastive%20pre%20training' target=\"_blank\">https://wandb.ai/zeihenri-hasso-plattner-institut/contrastive%20pre%20training</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20260225_213106-fjky2l8p/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tried to log to step 4 that is less than the current step 39. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.14)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
